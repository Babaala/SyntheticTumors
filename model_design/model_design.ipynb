{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/tanzl/code/githubdemo/SyntheticTumors/TumorGenerated\n",
      "/home/tanzl/code/githubdemo/SyntheticTumors/TumorGenerated\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from typing import Hashable, Mapping, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot  as plt\n",
    "# 打印并修改当前工作目录\n",
    "print(os.getcwd())  # 检查当前工作目录\n",
    "os.chdir('/home/tanzl/code/githubdemo/SyntheticTumors/TumorGenerated')\n",
    "print(os.getcwd())  #\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# 导入模块\n",
    "from TumorGenerated_2d import TumorGenerated\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predefined texture have generated.\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train\"\n",
    "output_folder = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train_augmented\"\n",
    "mask_folder = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train_augmented_mask\"\n",
    "transform = TumorGenerated(keys=['image', 'label'],\n",
    "                           prob=1,                                # 进行数据增强的概率\n",
    "                           tumor_prob = [0.2, 0.2, 0.2, 0.2, 0.2])  # ['tiny'(4), 'small'(8), 'medium'(16), 'large'(32), 'mix']不同大小肿瘤的概率,括号内为半径，mix是不同大小的都有\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9387/204800 [02:25<42:46, 76.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00191_slice_026.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19541/204800 [04:51<39:06, 78.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00101_slice_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 37903/204800 [09:17<46:12, 60.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00349_slice_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52950/204800 [12:54<30:02, 84.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00636_slice_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 80970/204800 [19:35<27:02, 76.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00715_slice_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 103729/204800 [24:58<21:52, 77.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00369_slice_193.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 105672/204800 [25:26<28:42, 57.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00223_slice_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 117240/204800 [28:10<23:18, 62.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00049_slice_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 118562/204800 [28:29<25:29, 56.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00112_slice_194.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 138169/204800 [33:10<20:31, 54.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00719_slice_028.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 156945/204800 [37:39<11:04, 72.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00731_slice_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 170590/204800 [40:51<07:30, 76.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00421_slice_200.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 171852/204800 [41:10<11:11, 49.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00355_slice_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 188919/204800 [45:15<03:51, 68.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation: 00291_slice_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204800/204800 [51:56<00:00, 65.71it/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(mask_folder):\n",
    "    os.makedirs(mask_folder)\n",
    "\n",
    "count = 0\n",
    "error_count = 0\n",
    "error_names = []\n",
    "\n",
    "for filename in tqdm(os.listdir(input_folder)):\n",
    "    if filename.endswith(\".png\"):\n",
    "        try:\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            image = np.array(Image.open(filepath).convert('L'))  # Convert to grayscale\n",
    "\n",
    "            if np.sum(image) == 0:\n",
    "                continue  # Skip empty images\n",
    "\n",
    "            label = deepcopy(image)  # Assuming a dummy label for simplicity\n",
    "            image = image/255\n",
    "            label[label > 0] = 1\n",
    "\n",
    "            data = {'image': image, 'label': label}\n",
    "            transformed_data = transform(data)\n",
    "            \n",
    "            transformed_image = Image.fromarray((transformed_data['image'] * 255).astype(np.uint8))\n",
    "            transformed_image.save(os.path.join(output_folder, filename.replace(\".png\", \"_augmented.png\")))\n",
    "            if np.sum(transformed_data['label']) == np.sum(label):\n",
    "                print(f\"No augmentation: {filename}\")\n",
    "                transformed_label = Image.fromarray((np.zeros_like(transformed_data['label'])).astype(np.uint8))\n",
    "                transformed_label.save(os.path.join(mask_folder, filename.replace(\".png\", \"_augmented_mask.png\")))\n",
    "            else:\n",
    "                transformed_label = Image.fromarray((transformed_data['label'] * 255).astype(np.uint8))\n",
    "                transformed_label.save(os.path.join(mask_folder, filename.replace(\".png\", \"_augmented_mask.png\")))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {filename}\")\n",
    "            error_names.append(filename)\n",
    "        # print(f\"Processed and saved: {filename}\")\n",
    "\n",
    "        # count += 1\n",
    "        # if count > 10:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 展示前后差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 配置各个目录路径\n",
    "\n",
    "\n",
    "original_dir = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/toy\"\n",
    "enhanced_dir = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/toy_augmented\"\n",
    "difference_dir = \"/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/toy_difference\"\n",
    "os.makedirs(difference_dir, exist_ok=True)\n",
    "\n",
    "def load_image_as_numpy(image_path):\n",
    "    \"\"\"加载图像并转换为 numpy 数组。\"\"\"\n",
    "    image = Image.open(image_path).convert('L')  # 转换为灰度图像\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "def save_image_from_numpy(image, save_path):\n",
    "    \"\"\"将 numpy 数组保存为图像。\"\"\"\n",
    "    image = Image.fromarray(image.astype(np.uint8))\n",
    "    image.save(save_path)\n",
    "\n",
    "# 获取所有原始图像文件\n",
    "image_files = [f for f in os.listdir(enhanced_dir) if f.endswith(\".png\")]\n",
    "\n",
    "for image_file in image_files:\n",
    "    original_image_path = os.path.join(original_dir, image_file)\n",
    "    enhanced_image_path = os.path.join(enhanced_dir, image_file)\n",
    "    \n",
    "    if not os.path.exists(enhanced_image_path):\n",
    "        print(f\"Enhanced image for {image_file} not found, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # 加载原始图像和增强图像\n",
    "    original_image = load_image_as_numpy(original_image_path)\n",
    "    enhanced_image = load_image_as_numpy(enhanced_image_path)\n",
    "    \n",
    "    # 确保图像尺寸一致\n",
    "    if original_image.shape != enhanced_image.shape:\n",
    "        print(f\"Image shapes do not match for {image_file}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # 计算差异\n",
    "    difference_image = np.abs(enhanced_image - original_image)\n",
    "    if np.sum(difference_image) > 0:\n",
    "        print(image_file)\n",
    "    \n",
    "    # 保存差异图像\n",
    "    difference_image_path = os.path.join(difference_dir, image_file)\n",
    "    save_image_from_numpy(difference_image, difference_image_path)\n",
    "\n",
    "print(\"Difference computation and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
